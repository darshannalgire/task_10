{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb136d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.0-cp310-cp310-win_amd64.whl.metadata (113 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.9 MB 509.0 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 479.5 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.3/8.9 MB 554.8 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 1.6/8.9 MB 625.9 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.6/8.9 MB 625.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.1/8.9 MB 584.4 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.1/8.9 MB 584.4 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 2.6/8.9 MB 522.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 2.6/8.9 MB 522.5 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 2.9/8.9 MB 543.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 2.9/8.9 MB 543.0 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 3.4/8.9 MB 541.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 3.7/8.9 MB 559.2 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 3.9/8.9 MB 570.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 3.9/8.9 MB 570.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.2/8.9 MB 585.2 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.2/8.9 MB 585.2 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 4.7/8.9 MB 568.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 586.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 598.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 598.1 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 5.5/8.9 MB 609.0 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 6.0/8.9 MB 610.1 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.0/8.9 MB 610.1 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 598.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 6.6/8.9 MB 616.6 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 6.8/8.9 MB 634.5 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 7.3/8.9 MB 628.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 639.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 7.9/8.9 MB 651.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 8.1/8.9 MB 664.9 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 8.4/8.9 MB 673.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.4/8.9 MB 673.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/8.9 MB 667.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 673.2 kB/s  0:00:13\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.1 MB 907.1 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.8/8.1 MB 907.1 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 751.1 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 751.1 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.1 MB 721.7 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.1 MB 721.7 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 1.8/8.1 MB 641.2 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 1.8/8.1 MB 641.2 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 638.3 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 638.3 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 615.7 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 615.7 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 2.6/8.1 MB 618.9 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.6/8.1 MB 618.9 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 2.9/8.1 MB 623.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 617.7 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 3.7/8.1 MB 635.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.7/8.1 MB 635.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.9/8.1 MB 641.8 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.9/8.1 MB 641.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 4.5/8.1 MB 628.6 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 4.5/8.1 MB 628.6 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 628.3 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 5.2/8.1 MB 626.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 5.5/8.1 MB 634.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 643.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 643.0 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 648.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 648.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 6.6/8.1 MB 670.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 6.6/8.1 MB 670.0 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 698.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 7.6/8.1 MB 731.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 731.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 738.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 745.0 kB/s  0:00:10\n",
      "Downloading pandas-2.3.2-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.3 MB 3.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.7/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.9/11.3 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.2/11.3 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.5/11.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.7/11.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.2/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.5/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.0/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.8/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.2/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.4/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.2/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 1.4 MB/s  0:00:07\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.4 MB/s  0:00:01\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/7.0 MB 1.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/7.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 985.5 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.3/7.0 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/7.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.6/7.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.1/7.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.4/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 4.7/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.2/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/7.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 1.2 MB/s  0:00:05\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, regex, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, click, scikit-learn, pandas, nltk, matplotlib\n",
      "\n",
      "   ----------------------------------------  0/17 [pytz]\n",
      "   -- -------------------------------------  1/17 [tzdata]\n",
      "   ------- --------------------------------  3/17 [threadpoolctl]\n",
      "   -------------- -------------------------  6/17 [pillow]\n",
      "   -------------- -------------------------  6/17 [pillow]\n",
      "   ------------------ ---------------------  8/17 [joblib]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ---------------------------------------- 17/17 [matplotlib]\n",
      "\n",
      "Successfully installed click-8.3.0 contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 nltk-3.9.1 pandas-2.3.2 pillow-11.3.0 pyparsing-3.2.5 pytz-2025.2 regex-2025.9.18 scikit-learn-1.7.2 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn nltk matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3cbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec tfenv in C:\\Users\\ASUS\\AppData\\Roaming\\jupyter\\kernels\\tfenv\n"
     ]
    }
   ],
   "source": [
    "!python -m ipykernel install --user --name tfenv --display-name \"Python (tfenv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25c118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "TensorFlow: 2.12.0\n",
      "NumPy: 1.23.5\n",
      "scikit-learn: 1.7.2\n",
      "NLTK: 3.9.1\n",
      "Matplotlib: 3.10.6\n",
      "Pandas: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "# Print versions\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"NLTK:\", nltk.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "329b84c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful! You're ready to run your AI/ML project.\n"
     ]
    }
   ],
   "source": [
    "# Test imports for your project\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "print(\"All imports successful! You're ready to run your AI/ML project.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f5551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70de282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How are you?</td>\n",
       "      <td>Comment ça va?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Je t'aime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good morning</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you</td>\n",
       "      <td>Merci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        English          French\n",
       "0         Hello         Bonjour\n",
       "1  How are you?  Comment ça va?\n",
       "2    I love you       Je t'aime\n",
       "3  Good morning         Bonjour\n",
       "4     Thank you           Merci"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tiny sample dataset\n",
    "data = {\n",
    "    \"English\": [\"Hello\", \"How are you?\", \"I love you\", \"Good morning\", \"Thank you\"],\n",
    "    \"French\": [\"Bonjour\", \"Comment ça va?\", \"Je t'aime\", \"Bonjour\", \"Merci\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"eng-fra.csv\", index=False)  # saves CSV in current folder\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910a7867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How are you?</td>\n",
       "      <td>Comment ça va?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Je t'aime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good morning</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you</td>\n",
       "      <td>Merci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        English          French\n",
       "0         Hello         Bonjour\n",
       "1  How are you?  Comment ça va?\n",
       "2    I love you       Je t'aime\n",
       "3  Good morning         Bonjour\n",
       "4     Thank you           Merci"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tiny sample dataset\n",
    "data = {\n",
    "    \"English\": [\"Hello\", \"How are you?\", \"I love you\", \"Good morning\", \"Thank you\"],\n",
    "    \"French\": [\"Bonjour\", \"Comment ça va?\", \"Je t'aime\", \"Bonjour\", \"Merci\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"eng-fra.csv\", index=False)  # saves CSV in current folder\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dfcfbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 10 Max length: 3\n",
      "French vocab size: 10 Max length: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Split English and French sentences\n",
    "english_sentences = df['English'].values\n",
    "french_sentences = df['French'].values\n",
    "\n",
    "# Add <start> and <end> tokens for French sentences\n",
    "french_sentences = ['<start> ' + sent + ' <end>' for sent in french_sentences]\n",
    "\n",
    "# Tokenizer for English\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "eng_padded = pad_sequences(eng_sequences, padding='post')\n",
    "\n",
    "# Tokenizer for French\n",
    "fra_tokenizer = Tokenizer(filters='')\n",
    "fra_tokenizer.fit_on_texts(french_sentences)\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_sequences = fra_tokenizer.texts_to_sequences(french_sentences)\n",
    "fra_padded = pad_sequences(fra_sequences, padding='post')\n",
    "\n",
    "# Maximum lengths\n",
    "max_eng_len = eng_padded.shape[1]\n",
    "max_fra_len = fra_padded.shape[1]\n",
    "\n",
    "print(\"English vocab size:\", eng_vocab_size, \"Max length:\", max_eng_len)\n",
    "print(\"French vocab size:\", fra_vocab_size, \"Max length:\", max_fra_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba47b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 10 Max length: 3\n",
      "French vocab size: 10 Max length: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Split English and French sentences\n",
    "english_sentences = df['English'].values\n",
    "french_sentences = df['French'].values\n",
    "\n",
    "# Add <start> and <end> tokens for French sentences\n",
    "french_sentences = ['<start> ' + sent + ' <end>' for sent in french_sentences]\n",
    "\n",
    "# Tokenizer for English\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "eng_padded = pad_sequences(eng_sequences, padding='post')\n",
    "\n",
    "# Tokenizer for French\n",
    "fra_tokenizer = Tokenizer(filters='')\n",
    "fra_tokenizer.fit_on_texts(french_sentences)\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_sequences = fra_tokenizer.texts_to_sequences(french_sentences)\n",
    "fra_padded = pad_sequences(fra_sequences, padding='post')\n",
    "\n",
    "# Maximum lengths\n",
    "max_eng_len = eng_padded.shape[1]\n",
    "max_fra_len = fra_padded.shape[1]\n",
    "\n",
    "print(\"English vocab size:\", eng_vocab_size, \"Max length:\", max_eng_len)\n",
    "print(\"French vocab size:\", fra_vocab_size, \"Max length:\", max_fra_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8430bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n"
     ]
    }
   ],
   "source": [
    "# Decoder input is French sentence without last token\n",
    "decoder_input = fra_padded[:, :-1]\n",
    "\n",
    "# Decoder output is French sentence shifted by one (teacher forcing)\n",
    "decoder_output = fra_padded[:, 1:]\n",
    "\n",
    "# One-hot encode decoder output\n",
    "decoder_output = tf.keras.utils.to_categorical(decoder_output, num_classes=fra_vocab_size)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, dec_in_train, dec_in_test = train_test_split(\n",
    "    eng_padded, decoder_output, decoder_input, test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9e2a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n"
     ]
    }
   ],
   "source": [
    "# Decoder input is French sentence without last token\n",
    "decoder_input = fra_padded[:, :-1]\n",
    "\n",
    "# Decoder output is French sentence shifted by one (teacher forcing)\n",
    "decoder_output = fra_padded[:, 1:]\n",
    "\n",
    "# One-hot encode decoder output\n",
    "decoder_output = tf.keras.utils.to_categorical(decoder_output, num_classes=fra_vocab_size)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, dec_in_train, dec_in_test = train_test_split(\n",
    "    eng_padded, decoder_output, decoder_input, test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f80d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 3, 64)        640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 4, 64)        640         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 3, 64),      33024       ['embedding[0][0]']              \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 4, 64),      33024       ['embedding_1[0][0]',            \n",
      "                                 (None, 64),                      'lstm[0][1]',                   \n",
      "                                 (None, 64)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 4, 64)        0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 128)       0           ['lstm_1[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4, 10)        1290        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 68,618\n",
      "Trainable params: 68,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
    "\n",
    "embedding_dim = 64\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "enc_emb = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_fra_len-1,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, embedding_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention = Attention()\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20f0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 3, 64)        640         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 4, 64)        640         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 3, 64),      33024       ['embedding_2[0][0]']            \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, 4, 64),      33024       ['embedding_3[0][0]',            \n",
      "                                 (None, 64),                      'lstm_2[0][1]',                 \n",
      "                                 (None, 64)]                      'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 4, 64)        0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 128)       0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4, 10)        1290        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 68,618\n",
      "Trainable params: 68,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
    "\n",
    "embedding_dim = 64\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "enc_emb = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_fra_len-1,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, embedding_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention = Attention()\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170ef48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['French'] = df['French'].apply(lambda x: '<start> ' + x + ' <end>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98015231",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_sequences_input = []\n",
    "fra_sequences_target = []\n",
    "\n",
    "for sentence in df['French']:\n",
    "    tokens = sentence.split()\n",
    "    fra_sequences_input.append(tokens[:-1])   # decoder input\n",
    "    fra_sequences_target.append(tokens[1:])   # decoder target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3673b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you already created the French tokenizer\n",
    "# fra_tokenizer = Tokenizer(filters='', lower=True)\n",
    "# fra_tokenizer.fit_on_texts(df['French'])\n",
    "\n",
    "# Convert to integer sequences\n",
    "fra_sequences_input = fra_tokenizer.texts_to_sequences([' '.join(seq) for seq in fra_sequences_input])\n",
    "fra_sequences_target = fra_tokenizer.texts_to_sequences([' '.join(seq) for seq in fra_sequences_target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1643ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "dec_in_train = pad_sequences(fra_sequences_input, maxlen=max_fra_len-1, padding='post')\n",
    "y_train = pad_sequences(fra_sequences_target, maxlen=max_fra_len-1, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7094378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode y_train\n",
    "y_train = to_categorical(y_train, num_classes=fra_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92f7a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=fra_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cce08b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
    "dec_in_train = pad_sequences(fra_sequences_input, maxlen=max_fra_len-1, padding='post')\n",
    "y_train = pad_sequences(fra_sequences_target, maxlen=max_fra_len-1, padding='post')\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=fra_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6196614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "190f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "latent_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78edf3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder input\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "enc_emb = Embedding(input_dim=eng_vocab_size, output_dim=embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5a07f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input\n",
    "decoder_inputs = Input(shape=(max_fra_len-1,))\n",
    "dec_emb = Embedding(input_dim=fra_vocab_size, output_dim=embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "\n",
    "# LSTM layer with initial state from encoder\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "211be532",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention()\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concatenate context vector and decoder outputs\n",
    "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, context_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "128cd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c182440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 3, 64)        640         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 4, 64)        640         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, 3, 64),      33024       ['embedding_4[0][0]']            \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 4, 64),      33024       ['embedding_5[0][0]',            \n",
      "                                 (None, 64),                      'lstm_4[0][1]',                 \n",
      "                                 (None, 64)]                      'lstm_4[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_2 (Attention)        (None, 4, 64)        0           ['lstm_5[0][0]',                 \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 4, 128)       0           ['lstm_5[0][0]',                 \n",
      "                                                                  'attention_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4, 10)        1290        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 68,618\n",
      "Trainable params: 68,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51000ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: tokenized French sentences\n",
    "fra_sequences_input = []\n",
    "fra_sequences_target = []\n",
    "\n",
    "for seq in fra_sequences:  # fra_sequences is your tokenized French sentences\n",
    "    fra_sequences_input.append(seq[:-1])   # remove last token\n",
    "    fra_sequences_target.append(seq[1:])   # remove first token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "346f92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encoder input\n",
    "X_train = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "# Decoder input\n",
    "dec_in_train = pad_sequences(fra_sequences_input, maxlen=max_fra_len-1, padding='post')\n",
    "\n",
    "# Decoder target\n",
    "y_train = pad_sequences(fra_sequences_target, maxlen=max_fra_len-1, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5643b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=fra_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25983d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 5s 5s/step - loss: 2.3068 - accuracy: 0.0000e+00 - val_loss: 2.3049 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2996 - accuracy: 0.2727 - val_loss: 2.3024 - val_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.2925 - accuracy: 0.3636 - val_loss: 2.2999 - val_accuracy: 0.5000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2852 - accuracy: 0.3636 - val_loss: 2.2973 - val_accuracy: 0.5000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2779 - accuracy: 0.3636 - val_loss: 2.2947 - val_accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2704 - accuracy: 0.3636 - val_loss: 2.2920 - val_accuracy: 0.5000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2626 - accuracy: 0.3636 - val_loss: 2.2893 - val_accuracy: 0.5000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2545 - accuracy: 0.3636 - val_loss: 2.2863 - val_accuracy: 0.5000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2460 - accuracy: 0.3636 - val_loss: 2.2833 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2370 - accuracy: 0.3636 - val_loss: 2.2801 - val_accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.2276 - accuracy: 0.3636 - val_loss: 2.2767 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2175 - accuracy: 0.3636 - val_loss: 2.2731 - val_accuracy: 0.5000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.2068 - accuracy: 0.3636 - val_loss: 2.2692 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.1953 - accuracy: 0.3636 - val_loss: 2.2651 - val_accuracy: 0.5000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1830 - accuracy: 0.3636 - val_loss: 2.2607 - val_accuracy: 0.5000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.1697 - accuracy: 0.3636 - val_loss: 2.2561 - val_accuracy: 0.5000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1554 - accuracy: 0.3636 - val_loss: 2.2510 - val_accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1399 - accuracy: 0.3636 - val_loss: 2.2456 - val_accuracy: 0.5000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1231 - accuracy: 0.3636 - val_loss: 2.2398 - val_accuracy: 0.5000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.1049 - accuracy: 0.3636 - val_loss: 2.2335 - val_accuracy: 0.5000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0851 - accuracy: 0.3636 - val_loss: 2.2268 - val_accuracy: 0.5000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0636 - accuracy: 0.3636 - val_loss: 2.2195 - val_accuracy: 0.5000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0402 - accuracy: 0.3636 - val_loss: 2.2116 - val_accuracy: 0.5000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0149 - accuracy: 0.3636 - val_loss: 2.2032 - val_accuracy: 0.5000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.9875 - accuracy: 0.3636 - val_loss: 2.1940 - val_accuracy: 0.5000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9579 - accuracy: 0.3636 - val_loss: 2.1842 - val_accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9262 - accuracy: 0.3636 - val_loss: 2.1737 - val_accuracy: 0.5000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8924 - accuracy: 0.3636 - val_loss: 2.1625 - val_accuracy: 0.5000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8569 - accuracy: 0.3636 - val_loss: 2.1508 - val_accuracy: 0.5000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8203 - accuracy: 0.3636 - val_loss: 2.1386 - val_accuracy: 0.5000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7834 - accuracy: 0.3636 - val_loss: 2.1261 - val_accuracy: 0.5000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7472 - accuracy: 0.3636 - val_loss: 2.1139 - val_accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7130 - accuracy: 0.3636 - val_loss: 2.1026 - val_accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6815 - accuracy: 0.3636 - val_loss: 2.0928 - val_accuracy: 0.5000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6530 - accuracy: 0.3636 - val_loss: 2.0856 - val_accuracy: 0.5000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6265 - accuracy: 0.3636 - val_loss: 2.0816 - val_accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6003 - accuracy: 0.3636 - val_loss: 2.0812 - val_accuracy: 0.5000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5729 - accuracy: 0.3636 - val_loss: 2.0846 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5428 - accuracy: 0.3636 - val_loss: 2.0914 - val_accuracy: 0.5000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5096 - accuracy: 0.3636 - val_loss: 2.1015 - val_accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4736 - accuracy: 0.3636 - val_loss: 2.1143 - val_accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4353 - accuracy: 0.3636 - val_loss: 2.1293 - val_accuracy: 0.5000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3956 - accuracy: 0.3636 - val_loss: 2.1460 - val_accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3552 - accuracy: 0.3636 - val_loss: 2.1638 - val_accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3147 - accuracy: 0.4545 - val_loss: 2.1823 - val_accuracy: 0.5000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2744 - accuracy: 0.4545 - val_loss: 2.2008 - val_accuracy: 0.5000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2342 - accuracy: 0.4545 - val_loss: 2.2188 - val_accuracy: 0.5000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1940 - accuracy: 0.5455 - val_loss: 2.2357 - val_accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1532 - accuracy: 0.6364 - val_loss: 2.2512 - val_accuracy: 0.5000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1114 - accuracy: 0.6364 - val_loss: 2.2648 - val_accuracy: 0.5000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0683 - accuracy: 0.7273 - val_loss: 2.2765 - val_accuracy: 0.5000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0238 - accuracy: 0.7273 - val_loss: 2.2860 - val_accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9780 - accuracy: 0.7273 - val_loss: 2.2935 - val_accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9315 - accuracy: 0.7273 - val_loss: 2.2992 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8849 - accuracy: 0.8182 - val_loss: 2.3033 - val_accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8391 - accuracy: 0.8182 - val_loss: 2.3063 - val_accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7951 - accuracy: 0.8182 - val_loss: 2.3086 - val_accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7535 - accuracy: 0.8182 - val_loss: 2.3108 - val_accuracy: 0.5000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7145 - accuracy: 0.8182 - val_loss: 2.3133 - val_accuracy: 0.5000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6781 - accuracy: 0.9091 - val_loss: 2.3165 - val_accuracy: 0.5000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 2.3207 - val_accuracy: 0.5000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6106 - accuracy: 1.0000 - val_loss: 2.3257 - val_accuracy: 0.5000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5789 - accuracy: 1.0000 - val_loss: 2.3314 - val_accuracy: 0.5000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5487 - accuracy: 1.0000 - val_loss: 2.3374 - val_accuracy: 0.5000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5202 - accuracy: 1.0000 - val_loss: 2.3434 - val_accuracy: 0.5000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4938 - accuracy: 1.0000 - val_loss: 2.3487 - val_accuracy: 0.5000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4693 - accuracy: 1.0000 - val_loss: 2.3534 - val_accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4462 - accuracy: 1.0000 - val_loss: 2.3574 - val_accuracy: 0.5000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4239 - accuracy: 1.0000 - val_loss: 2.3611 - val_accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4026 - accuracy: 1.0000 - val_loss: 2.3650 - val_accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3825 - accuracy: 1.0000 - val_loss: 2.3698 - val_accuracy: 0.5000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3638 - accuracy: 1.0000 - val_loss: 2.3756 - val_accuracy: 0.5000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3463 - accuracy: 1.0000 - val_loss: 2.3827 - val_accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3297 - accuracy: 1.0000 - val_loss: 2.3907 - val_accuracy: 0.5000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3138 - accuracy: 1.0000 - val_loss: 2.3993 - val_accuracy: 0.5000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2986 - accuracy: 1.0000 - val_loss: 2.4081 - val_accuracy: 0.5000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2843 - accuracy: 1.0000 - val_loss: 2.4166 - val_accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2710 - accuracy: 1.0000 - val_loss: 2.4246 - val_accuracy: 0.5000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2583 - accuracy: 1.0000 - val_loss: 2.4323 - val_accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2460 - accuracy: 1.0000 - val_loss: 2.4397 - val_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2342 - accuracy: 1.0000 - val_loss: 2.4473 - val_accuracy: 0.5000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2231 - accuracy: 1.0000 - val_loss: 2.4554 - val_accuracy: 0.5000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2128 - accuracy: 1.0000 - val_loss: 2.4641 - val_accuracy: 0.5000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2030 - accuracy: 1.0000 - val_loss: 2.4735 - val_accuracy: 0.5000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1938 - accuracy: 1.0000 - val_loss: 2.4834 - val_accuracy: 0.5000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1849 - accuracy: 1.0000 - val_loss: 2.4938 - val_accuracy: 0.5000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1766 - accuracy: 1.0000 - val_loss: 2.5043 - val_accuracy: 0.5000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1689 - accuracy: 1.0000 - val_loss: 2.5150 - val_accuracy: 0.5000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1617 - accuracy: 1.0000 - val_loss: 2.5257 - val_accuracy: 0.5000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1548 - accuracy: 1.0000 - val_loss: 2.5365 - val_accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1483 - accuracy: 1.0000 - val_loss: 2.5474 - val_accuracy: 0.5000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1421 - accuracy: 1.0000 - val_loss: 2.5584 - val_accuracy: 0.5000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1362 - accuracy: 1.0000 - val_loss: 2.5695 - val_accuracy: 0.5000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 2.5808 - val_accuracy: 0.5000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1253 - accuracy: 1.0000 - val_loss: 2.5920 - val_accuracy: 0.5000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1201 - accuracy: 1.0000 - val_loss: 2.6033 - val_accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 2.6146 - val_accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1106 - accuracy: 1.0000 - val_loss: 2.6258 - val_accuracy: 0.5000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1062 - accuracy: 1.0000 - val_loss: 2.6368 - val_accuracy: 0.5000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1020 - accuracy: 1.0000 - val_loss: 2.6477 - val_accuracy: 0.5000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0980 - accuracy: 1.0000 - val_loss: 2.6585 - val_accuracy: 0.5000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0942 - accuracy: 1.0000 - val_loss: 2.6691 - val_accuracy: 0.5000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 2.6796 - val_accuracy: 0.5000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0871 - accuracy: 1.0000 - val_loss: 2.6900 - val_accuracy: 0.5000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 2.7002 - val_accuracy: 0.5000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0807 - accuracy: 1.0000 - val_loss: 2.7103 - val_accuracy: 0.5000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0778 - accuracy: 1.0000 - val_loss: 2.7203 - val_accuracy: 0.5000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 2.7301 - val_accuracy: 0.5000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0723 - accuracy: 1.0000 - val_loss: 2.7398 - val_accuracy: 0.5000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0698 - accuracy: 1.0000 - val_loss: 2.7494 - val_accuracy: 0.5000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0674 - accuracy: 1.0000 - val_loss: 2.7589 - val_accuracy: 0.5000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 2.7683 - val_accuracy: 0.5000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 2.7775 - val_accuracy: 0.5000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 2.7866 - val_accuracy: 0.5000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 2.7956 - val_accuracy: 0.5000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 2.8044 - val_accuracy: 0.5000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 2.8131 - val_accuracy: 0.5000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0533 - accuracy: 1.0000 - val_loss: 2.8217 - val_accuracy: 0.5000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 2.8301 - val_accuracy: 0.5000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 2.8384 - val_accuracy: 0.5000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 2.8466 - val_accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 2.8547 - val_accuracy: 0.5000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 2.8626 - val_accuracy: 0.5000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 2.8705 - val_accuracy: 0.5000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 2.8782 - val_accuracy: 0.5000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.8858 - val_accuracy: 0.5000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 2.8933 - val_accuracy: 0.5000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 2.9007 - val_accuracy: 0.5000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 2.9080 - val_accuracy: 0.5000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 2.9152 - val_accuracy: 0.5000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 2.9223 - val_accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 2.9293 - val_accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 2.9362 - val_accuracy: 0.5000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 2.9430 - val_accuracy: 0.5000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 2.9498 - val_accuracy: 0.5000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 2.9564 - val_accuracy: 0.5000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 2.9629 - val_accuracy: 0.5000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 2.9693 - val_accuracy: 0.5000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 2.9756 - val_accuracy: 0.5000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.9819 - val_accuracy: 0.5000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 2.9880 - val_accuracy: 0.5000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 2.9941 - val_accuracy: 0.5000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.0000 - val_accuracy: 0.5000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 3.0059 - val_accuracy: 0.5000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 3.0117 - val_accuracy: 0.5000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 3.0174 - val_accuracy: 0.5000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 3.0230 - val_accuracy: 0.5000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 3.0285 - val_accuracy: 0.5000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 3.0340 - val_accuracy: 0.5000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.0394 - val_accuracy: 0.5000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.0447 - val_accuracy: 0.5000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.0499 - val_accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.0550 - val_accuracy: 0.5000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 3.0601 - val_accuracy: 0.5000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 3.0651 - val_accuracy: 0.5000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 3.0701 - val_accuracy: 0.5000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 3.0750 - val_accuracy: 0.5000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 3.0798 - val_accuracy: 0.5000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.0845 - val_accuracy: 0.5000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 3.0892 - val_accuracy: 0.5000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 3.0938 - val_accuracy: 0.5000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 3.0984 - val_accuracy: 0.5000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 3.1029 - val_accuracy: 0.5000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 3.1073 - val_accuracy: 0.5000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 3.1117 - val_accuracy: 0.5000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 3.1160 - val_accuracy: 0.5000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 3.1203 - val_accuracy: 0.5000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 3.1245 - val_accuracy: 0.5000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 3.1286 - val_accuracy: 0.5000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 3.1327 - val_accuracy: 0.5000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.1368 - val_accuracy: 0.5000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.1408 - val_accuracy: 0.5000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 3.1447 - val_accuracy: 0.5000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.1486 - val_accuracy: 0.5000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 3.1525 - val_accuracy: 0.5000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.1563 - val_accuracy: 0.5000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.1601 - val_accuracy: 0.5000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 3.1638 - val_accuracy: 0.5000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 3.1675 - val_accuracy: 0.5000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 3.1711 - val_accuracy: 0.5000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 3.1747 - val_accuracy: 0.5000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 3.1782 - val_accuracy: 0.5000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 3.1817 - val_accuracy: 0.5000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 3.1852 - val_accuracy: 0.5000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.1886 - val_accuracy: 0.5000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 3.1920 - val_accuracy: 0.5000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 3.1954 - val_accuracy: 0.5000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.1987 - val_accuracy: 0.5000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 3.2020 - val_accuracy: 0.5000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.2052 - val_accuracy: 0.5000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.2084 - val_accuracy: 0.5000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 3.2116 - val_accuracy: 0.5000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 3.2147 - val_accuracy: 0.5000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.2178 - val_accuracy: 0.5000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.2209 - val_accuracy: 0.5000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.2239 - val_accuracy: 0.5000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.2270 - val_accuracy: 0.5000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.2299 - val_accuracy: 0.5000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.2329 - val_accuracy: 0.5000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.2358 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, dec_in_train],\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03dec8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('eng_fra_translator.h5')\n",
    "\n",
    "import pickle\n",
    "with open('english_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(eng_tokenizer, f)\n",
    "with open('french_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(fra_tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51835793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# --- Encoder inference ---\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# --- Decoder inference ---\n",
    "# Decoder inputs\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "encoder_output_input = Input(shape=(max_eng_len, latent_dim))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "    dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "# Attention\n",
    "context_vector2 = Attention()([decoder_outputs2, encoder_output_input])\n",
    "decoder_concat2 = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs2, context_vector2])\n",
    "\n",
    "# Dense output\n",
    "decoder_outputs2 = decoder_dense(decoder_concat2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, decoder_state_input_h, decoder_state_input_c, encoder_output_input],\n",
    "    [decoder_outputs2, state_h2, state_c2]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8f8cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def translate_sentence(input_seq):\n",
    "    # Encode the input sentence\n",
    "    enc_outs, enc_h, enc_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Start token for decoder\n",
    "    target_seq = np.array([[fra_tokenizer.word_index['<start>']]])\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, enc_h, enc_c, enc_outs])\n",
    "        \n",
    "        # Get the token with highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = fra_tokenizer.index_word.get(sampled_token_index, '')\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_fra_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "            \n",
    "            # Update the target sequence\n",
    "            target_seq = np.array([[sampled_token_index]])\n",
    "            \n",
    "            # Update states\n",
    "            enc_h, enc_c = h, c\n",
    "    \n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9ed265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "English: Hello\n",
      "French: bonjour\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Hello\"\n",
    "test_seq = pad_sequences(eng_tokenizer.texts_to_sequences([test_sentence]), maxlen=max_eng_len, padding='post')\n",
    "translated = translate_sentence(test_seq)\n",
    "print(\"English:\", test_sentence)\n",
    "print(\"French:\", translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497a076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07804672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save the trained model\n",
    "model.save('eng_fra_translator.h5')\n",
    "\n",
    "# Save tokenizers\n",
    "with open('english_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(eng_tokenizer, f)\n",
    "\n",
    "with open('french_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(fra_tokenizer, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdfc1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model('eng_fra_translator.h5')\n",
    "\n",
    "# Load tokenizers\n",
    "with open('english_tokenizer.pkl', 'rb') as f:\n",
    "    eng_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('french_tokenizer.pkl', 'rb') as f:\n",
    "    fra_tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56b9c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(max_eng_len)  # should be 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
