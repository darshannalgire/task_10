{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb136d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.0-cp310-cp310-win_amd64.whl.metadata (113 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\tf_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.9 MB 509.0 kB/s eta 0:00:17\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.8/8.9 MB 699.0 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 479.5 kB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 1.3/8.9 MB 554.8 kB/s eta 0:00:14\n",
      "   ------- -------------------------------- 1.6/8.9 MB 625.9 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 1.6/8.9 MB 625.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 1.8/8.9 MB 595.8 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.1/8.9 MB 584.4 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.1/8.9 MB 584.4 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.4/8.9 MB 588.8 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 2.6/8.9 MB 522.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 2.6/8.9 MB 522.5 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 2.9/8.9 MB 543.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 2.9/8.9 MB 543.0 kB/s eta 0:00:12\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.1/8.9 MB 538.1 kB/s eta 0:00:11\n",
      "   --------------- ------------------------ 3.4/8.9 MB 541.2 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 3.7/8.9 MB 559.2 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 3.9/8.9 MB 570.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 3.9/8.9 MB 570.1 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.2/8.9 MB 585.2 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.2/8.9 MB 585.2 kB/s eta 0:00:09\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 4.5/8.9 MB 588.7 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 4.7/8.9 MB 568.1 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 586.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 598.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 598.1 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 5.5/8.9 MB 609.0 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 5.8/8.9 MB 611.7 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 6.0/8.9 MB 610.1 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.0/8.9 MB 610.1 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 6.3/8.9 MB 598.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 6.6/8.9 MB 616.6 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 6.8/8.9 MB 634.5 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.1/8.9 MB 645.3 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 7.3/8.9 MB 628.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 639.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 7.9/8.9 MB 651.3 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 8.1/8.9 MB 664.9 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 8.4/8.9 MB 673.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.4/8.9 MB 673.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/8.9 MB 667.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 673.2 kB/s  0:00:13\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/8.1 MB 1.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/8.1 MB 907.1 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.8/8.1 MB 907.1 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 751.1 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.0/8.1 MB 751.1 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.1 MB 721.7 kB/s eta 0:00:10\n",
      "   ------ --------------------------------- 1.3/8.1 MB 721.7 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 1.6/8.1 MB 723.4 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 1.8/8.1 MB 641.2 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 1.8/8.1 MB 641.2 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 638.3 kB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 638.3 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 615.7 kB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 615.7 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 2.6/8.1 MB 618.9 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 2.6/8.1 MB 618.9 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 2.9/8.1 MB 623.7 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.1/8.1 MB 645.4 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 617.7 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 3.7/8.1 MB 635.9 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 3.7/8.1 MB 635.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.9/8.1 MB 641.8 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 3.9/8.1 MB 641.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 4.2/8.1 MB 633.9 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 4.5/8.1 MB 628.6 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 4.5/8.1 MB 628.6 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 628.3 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 5.0/8.1 MB 639.8 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 5.2/8.1 MB 626.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 5.5/8.1 MB 634.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 643.0 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 643.0 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 648.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 648.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 6.6/8.1 MB 670.0 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 6.6/8.1 MB 670.0 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 698.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 7.6/8.1 MB 731.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 731.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 738.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 745.0 kB/s  0:00:10\n",
      "Downloading pandas-2.3.2-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.3 MB 3.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/11.3 MB 2.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.1/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.7/11.3 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.9/11.3 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.2/11.3 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.5/11.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.7/11.3 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.2/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.5/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.8/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.0/11.3 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 6.8/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 7.1/11.3 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.3 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.1/11.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.7/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.9/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.2/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.4/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.2/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 1.4 MB/s  0:00:07\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.3 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 1.4 MB/s  0:00:01\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pillow-11.3.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/7.0 MB 1.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/7.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 985.5 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.3/7.0 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/7.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/7.0 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.6/7.0 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.1/7.0 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.4/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.7/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 4.7/7.0 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.2/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.2/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.0 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/7.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 1.2 MB/s  0:00:05\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.9.18-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, regex, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, click, scikit-learn, pandas, nltk, matplotlib\n",
      "\n",
      "   ----------------------------------------  0/17 [pytz]\n",
      "   -- -------------------------------------  1/17 [tzdata]\n",
      "   ------- --------------------------------  3/17 [threadpoolctl]\n",
      "   -------------- -------------------------  6/17 [pillow]\n",
      "   -------------- -------------------------  6/17 [pillow]\n",
      "   ------------------ ---------------------  8/17 [joblib]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   --------------------- ------------------  9/17 [fonttools]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   ------------------------------ --------- 13/17 [scikit-learn]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   -------------------------------- ------- 14/17 [pandas]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ----------------------------------- ---- 15/17 [nltk]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ------------------------------------- -- 16/17 [matplotlib]\n",
      "   ---------------------------------------- 17/17 [matplotlib]\n",
      "\n",
      "Successfully installed click-8.3.0 contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 nltk-3.9.1 pandas-2.3.2 pillow-11.3.0 pyparsing-3.2.5 pytz-2025.2 regex-2025.9.18 scikit-learn-1.7.2 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn nltk matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3cbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec tfenv in C:\\Users\\ASUS\\AppData\\Roaming\\jupyter\\kernels\\tfenv\n"
     ]
    }
   ],
   "source": [
    "!python -m ipykernel install --user --name tfenv --display-name \"Python (tfenv)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c25c118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "TensorFlow: 2.12.0\n",
      "NumPy: 1.23.5\n",
      "scikit-learn: 1.7.2\n",
      "NLTK: 3.9.1\n",
      "Matplotlib: 3.10.6\n",
      "Pandas: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "# Print versions\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"NLTK:\", nltk.__version__)\n",
    "print(\"Matplotlib:\", matplotlib.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "329b84c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful! You're ready to run your AI/ML project.\n"
     ]
    }
   ],
   "source": [
    "# Test imports for your project\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "print(\"All imports successful! You're ready to run your AI/ML project.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f5551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70de282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How are you?</td>\n",
       "      <td>Comment ça va?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Je t'aime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good morning</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you</td>\n",
       "      <td>Merci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        English          French\n",
       "0         Hello         Bonjour\n",
       "1  How are you?  Comment ça va?\n",
       "2    I love you       Je t'aime\n",
       "3  Good morning         Bonjour\n",
       "4     Thank you           Merci"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tiny sample dataset\n",
    "data = {\n",
    "    \"English\": [\"Hello\", \"How are you?\", \"I love you\", \"Good morning\", \"Thank you\"],\n",
    "    \"French\": [\"Bonjour\", \"Comment ça va?\", \"Je t'aime\", \"Bonjour\", \"Merci\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"eng-fra.csv\", index=False)  # saves CSV in current folder\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dfcfbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 10 Max length: 3\n",
      "French vocab size: 10 Max length: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Split English and French sentences\n",
    "english_sentences = df['English'].values\n",
    "french_sentences = df['French'].values\n",
    "\n",
    "# Add <start> and <end> tokens for French sentences\n",
    "french_sentences = ['<start> ' + sent + ' <end>' for sent in french_sentences]\n",
    "\n",
    "# Tokenizer for English\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "eng_padded = pad_sequences(eng_sequences, padding='post')\n",
    "\n",
    "# Tokenizer for French\n",
    "fra_tokenizer = Tokenizer(filters='')\n",
    "fra_tokenizer.fit_on_texts(french_sentences)\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_sequences = fra_tokenizer.texts_to_sequences(french_sentences)\n",
    "fra_padded = pad_sequences(fra_sequences, padding='post')\n",
    "\n",
    "# Maximum lengths\n",
    "max_eng_len = eng_padded.shape[1]\n",
    "max_fra_len = fra_padded.shape[1]\n",
    "\n",
    "print(\"English vocab size:\", eng_vocab_size, \"Max length:\", max_eng_len)\n",
    "print(\"French vocab size:\", fra_vocab_size, \"Max length:\", max_fra_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8430bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n"
     ]
    }
   ],
   "source": [
    "# Decoder input is French sentence without last token\n",
    "decoder_input = fra_padded[:, :-1]\n",
    "\n",
    "# Decoder output is French sentence shifted by one (teacher forcing)\n",
    "decoder_output = fra_padded[:, 1:]\n",
    "\n",
    "# One-hot encode decoder output\n",
    "decoder_output = tf.keras.utils.to_categorical(decoder_output, num_classes=fra_vocab_size)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, dec_in_train, dec_in_test = train_test_split(\n",
    "    eng_padded, decoder_output, decoder_input, test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f80d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 3, 64)        640         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 4, 64)        640         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 3, 64),      33024       ['embedding[0][0]']              \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 4, 64),      33024       ['embedding_1[0][0]',            \n",
      "                                 (None, 64),                      'lstm[0][1]',                   \n",
      "                                 (None, 64)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 4, 64)        0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 128)       0           ['lstm_1[0][0]',                 \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4, 10)        1290        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 68,618\n",
      "Trainable params: 68,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
    "\n",
    "embedding_dim = 64\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "enc_emb = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_fra_len-1,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, embedding_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention = Attention()\n",
    "context_vector = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f92721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/2 [==============================] - 6s 1s/step - loss: 2.3037 - accuracy: 0.0909 - val_loss: 2.2970 - val_accuracy: 0.3333\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 2.2940 - accuracy: 0.2727 - val_loss: 2.2915 - val_accuracy: 0.6667\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.2835 - accuracy: 0.5455 - val_loss: 2.2867 - val_accuracy: 0.6667\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.2749 - accuracy: 0.5455 - val_loss: 2.2812 - val_accuracy: 0.6667\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.2649 - accuracy: 0.4545 - val_loss: 2.2753 - val_accuracy: 0.6667\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.2535 - accuracy: 0.4545 - val_loss: 2.2689 - val_accuracy: 0.3333\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.2419 - accuracy: 0.4545 - val_loss: 2.2614 - val_accuracy: 0.3333\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.2310 - accuracy: 0.4545 - val_loss: 2.2530 - val_accuracy: 0.3333\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 2.2170 - accuracy: 0.4545 - val_loss: 2.2445 - val_accuracy: 0.3333\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 2.2029 - accuracy: 0.3636 - val_loss: 2.2354 - val_accuracy: 0.3333\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.1851 - accuracy: 0.3636 - val_loss: 2.2247 - val_accuracy: 0.3333\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.1650 - accuracy: 0.2727 - val_loss: 2.2130 - val_accuracy: 0.3333\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 2.1438 - accuracy: 0.2727 - val_loss: 2.1988 - val_accuracy: 0.3333\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.1180 - accuracy: 0.2727 - val_loss: 2.1830 - val_accuracy: 0.3333\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.0893 - accuracy: 0.2727 - val_loss: 2.1639 - val_accuracy: 0.3333\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 2.0575 - accuracy: 0.2727 - val_loss: 2.1426 - val_accuracy: 0.3333\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 2.0173 - accuracy: 0.2727 - val_loss: 2.1166 - val_accuracy: 0.3333\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.9717 - accuracy: 0.2727 - val_loss: 2.0853 - val_accuracy: 0.3333\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.9213 - accuracy: 0.2727 - val_loss: 2.0489 - val_accuracy: 0.3333\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.8629 - accuracy: 0.2727 - val_loss: 2.0065 - val_accuracy: 0.3333\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.8067 - accuracy: 0.2727 - val_loss: 1.9605 - val_accuracy: 0.3333\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.7469 - accuracy: 0.2727 - val_loss: 1.9146 - val_accuracy: 0.3333\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6958 - accuracy: 0.2727 - val_loss: 1.8763 - val_accuracy: 0.3333\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6576 - accuracy: 0.2727 - val_loss: 1.8436 - val_accuracy: 0.3333\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.6149 - accuracy: 0.2727 - val_loss: 1.8264 - val_accuracy: 0.3333\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.5661 - accuracy: 0.2727 - val_loss: 1.8215 - val_accuracy: 0.3333\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.5211 - accuracy: 0.3636 - val_loss: 1.8249 - val_accuracy: 0.3333\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.4650 - accuracy: 0.4545 - val_loss: 1.8256 - val_accuracy: 0.3333\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.4045 - accuracy: 0.5455 - val_loss: 1.8235 - val_accuracy: 0.6667\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.3600 - accuracy: 0.5455 - val_loss: 1.8249 - val_accuracy: 0.6667\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.3005 - accuracy: 0.6364 - val_loss: 1.8273 - val_accuracy: 0.6667\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.2426 - accuracy: 0.7273 - val_loss: 1.8251 - val_accuracy: 0.6667\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.1778 - accuracy: 0.7273 - val_loss: 1.8243 - val_accuracy: 0.6667\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 1.1255 - accuracy: 0.7273 - val_loss: 1.8204 - val_accuracy: 0.6667\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 1.0496 - accuracy: 0.8182 - val_loss: 1.8140 - val_accuracy: 0.6667\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.9851 - accuracy: 0.8182 - val_loss: 1.8086 - val_accuracy: 0.6667\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.9383 - accuracy: 0.8182 - val_loss: 1.8053 - val_accuracy: 0.6667\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8691 - accuracy: 0.8182 - val_loss: 1.8025 - val_accuracy: 0.6667\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.8158 - accuracy: 0.9091 - val_loss: 1.8039 - val_accuracy: 0.6667\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7799 - accuracy: 0.9091 - val_loss: 1.8066 - val_accuracy: 0.6667\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7220 - accuracy: 0.9091 - val_loss: 1.8057 - val_accuracy: 0.6667\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6798 - accuracy: 0.9091 - val_loss: 1.7992 - val_accuracy: 0.6667\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.6525 - accuracy: 0.9091 - val_loss: 1.7874 - val_accuracy: 0.6667\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6009 - accuracy: 0.9091 - val_loss: 1.7745 - val_accuracy: 0.6667\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5807 - accuracy: 0.9091 - val_loss: 1.7625 - val_accuracy: 0.6667\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5477 - accuracy: 0.9091 - val_loss: 1.7552 - val_accuracy: 0.6667\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5114 - accuracy: 0.9091 - val_loss: 1.7526 - val_accuracy: 0.6667\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4835 - accuracy: 0.9091 - val_loss: 1.7531 - val_accuracy: 0.6667\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.4569 - accuracy: 0.9091 - val_loss: 1.7533 - val_accuracy: 0.6667\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4321 - accuracy: 0.9091 - val_loss: 1.7509 - val_accuracy: 0.6667\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.4046 - accuracy: 0.9091 - val_loss: 1.7450 - val_accuracy: 0.6667\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.3881 - accuracy: 1.0000 - val_loss: 1.7385 - val_accuracy: 0.6667\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3625 - accuracy: 1.0000 - val_loss: 1.7329 - val_accuracy: 0.6667\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3481 - accuracy: 1.0000 - val_loss: 1.7306 - val_accuracy: 0.6667\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3332 - accuracy: 1.0000 - val_loss: 1.7299 - val_accuracy: 0.6667\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3069 - accuracy: 1.0000 - val_loss: 1.7299 - val_accuracy: 0.6667\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2941 - accuracy: 1.0000 - val_loss: 1.7304 - val_accuracy: 0.6667\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2801 - accuracy: 1.0000 - val_loss: 1.7311 - val_accuracy: 0.6667\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2638 - accuracy: 1.0000 - val_loss: 1.7313 - val_accuracy: 0.6667\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2430 - accuracy: 1.0000 - val_loss: 1.7310 - val_accuracy: 0.6667\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2337 - accuracy: 1.0000 - val_loss: 1.7311 - val_accuracy: 0.6667\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2156 - accuracy: 1.0000 - val_loss: 1.7306 - val_accuracy: 0.6667\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2067 - accuracy: 1.0000 - val_loss: 1.7308 - val_accuracy: 0.6667\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1954 - accuracy: 1.0000 - val_loss: 1.7313 - val_accuracy: 0.6667\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1842 - accuracy: 1.0000 - val_loss: 1.7325 - val_accuracy: 0.6667\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1710 - accuracy: 1.0000 - val_loss: 1.7346 - val_accuracy: 0.6667\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1615 - accuracy: 1.0000 - val_loss: 1.7376 - val_accuracy: 0.6667\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1528 - accuracy: 1.0000 - val_loss: 1.7411 - val_accuracy: 0.6667\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1476 - accuracy: 1.0000 - val_loss: 1.7449 - val_accuracy: 0.6667\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1372 - accuracy: 1.0000 - val_loss: 1.7489 - val_accuracy: 0.6667\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1301 - accuracy: 1.0000 - val_loss: 1.7533 - val_accuracy: 0.6667\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1261 - accuracy: 1.0000 - val_loss: 1.7581 - val_accuracy: 0.6667\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 1.7627 - val_accuracy: 0.6667\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1136 - accuracy: 1.0000 - val_loss: 1.7675 - val_accuracy: 0.6667\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1082 - accuracy: 1.0000 - val_loss: 1.7723 - val_accuracy: 0.6667\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1012 - accuracy: 1.0000 - val_loss: 1.7769 - val_accuracy: 0.6667\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 1.7818 - val_accuracy: 0.6667\n",
      "Epoch 78/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0935 - accuracy: 1.0000 - val_loss: 1.7868 - val_accuracy: 0.6667\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0892 - accuracy: 1.0000 - val_loss: 1.7915 - val_accuracy: 0.6667\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 1.7961 - val_accuracy: 0.6667\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 1.8007 - val_accuracy: 0.6667\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0775 - accuracy: 1.0000 - val_loss: 1.8052 - val_accuracy: 0.6667\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0741 - accuracy: 1.0000 - val_loss: 1.8096 - val_accuracy: 0.6667\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0701 - accuracy: 1.0000 - val_loss: 1.8140 - val_accuracy: 0.6667\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0671 - accuracy: 1.0000 - val_loss: 1.8184 - val_accuracy: 0.6667\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 1.8229 - val_accuracy: 0.6667\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 1.8272 - val_accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 1.8314 - val_accuracy: 0.6667\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 1.8355 - val_accuracy: 0.6667\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 1.8394 - val_accuracy: 0.6667\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 1.8433 - val_accuracy: 0.6667\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 1.8472 - val_accuracy: 0.6667\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 1.8511 - val_accuracy: 0.6667\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 1.8551 - val_accuracy: 0.6667\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 1.8589 - val_accuracy: 0.6667\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 1.8626 - val_accuracy: 0.6667\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 1.8663 - val_accuracy: 0.6667\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 1.8700 - val_accuracy: 0.6667\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0404 - accuracy: 1.0000 - val_loss: 1.8737 - val_accuracy: 0.6667\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 1.8773 - val_accuracy: 0.6667\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 1.8808 - val_accuracy: 0.6667\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 1.8844 - val_accuracy: 0.6667\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 1.8880 - val_accuracy: 0.6667\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 1.8915 - val_accuracy: 0.6667\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 1.8949 - val_accuracy: 0.6667\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 1.8984 - val_accuracy: 0.6667\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 1.9019 - val_accuracy: 0.6667\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 0.6667\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 0.6667\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 1.9117 - val_accuracy: 0.6667\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 1.9147 - val_accuracy: 0.6667\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 1.9178 - val_accuracy: 0.6667\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 1.9208 - val_accuracy: 0.6667\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 1.9237 - val_accuracy: 0.6667\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 1.9267 - val_accuracy: 0.6667\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 1.9296 - val_accuracy: 0.6667\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 1.9324 - val_accuracy: 0.6667\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 1.9352 - val_accuracy: 0.6667\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 1.9379 - val_accuracy: 0.6667\n",
      "Epoch 120/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 1.9406 - val_accuracy: 0.6667\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 1.9434 - val_accuracy: 0.6667\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 1.9460 - val_accuracy: 0.6667\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 1.9487 - val_accuracy: 0.6667\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 1.9514 - val_accuracy: 0.6667\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.9541 - val_accuracy: 0.6667\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 1.9568 - val_accuracy: 0.6667\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 1.9595 - val_accuracy: 0.6667\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.9621 - val_accuracy: 0.6667\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 1.9647 - val_accuracy: 0.6667\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.9672 - val_accuracy: 0.6667\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 1.9696 - val_accuracy: 0.6667\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.9719 - val_accuracy: 0.6667\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 1.9743 - val_accuracy: 0.6667\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.9765 - val_accuracy: 0.6667\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.9789 - val_accuracy: 0.6667\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.9812 - val_accuracy: 0.6667\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.9835 - val_accuracy: 0.6667\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.9859 - val_accuracy: 0.6667\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 1.9881 - val_accuracy: 0.6667\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 1.9904 - val_accuracy: 0.6667\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.9926 - val_accuracy: 0.6667\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.9949 - val_accuracy: 0.6667\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.9971 - val_accuracy: 0.6667\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 1.9994 - val_accuracy: 0.6667\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 2.0016 - val_accuracy: 0.6667\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 2.0038 - val_accuracy: 0.6667\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 2.0060 - val_accuracy: 0.6667\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 2.0081 - val_accuracy: 0.6667\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.0103 - val_accuracy: 0.6667\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 2.0124 - val_accuracy: 0.6667\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.0145 - val_accuracy: 0.6667\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.0166 - val_accuracy: 0.6667\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 2.0187 - val_accuracy: 0.6667\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 2.0208 - val_accuracy: 0.6667\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 2.0228 - val_accuracy: 0.6667\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 2.0248 - val_accuracy: 0.6667\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 2.0267 - val_accuracy: 0.6667\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.0286 - val_accuracy: 0.6667\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.0305 - val_accuracy: 0.6667\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.0323 - val_accuracy: 0.6667\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.0342 - val_accuracy: 0.6667\n",
      "Epoch 162/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 2.0360 - val_accuracy: 0.6667\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.0378 - val_accuracy: 0.6667\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.0396 - val_accuracy: 0.6667\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.0414 - val_accuracy: 0.6667\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 2.0431 - val_accuracy: 0.6667\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.0449 - val_accuracy: 0.6667\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.0467 - val_accuracy: 0.6667\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.0485 - val_accuracy: 0.6667\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.0502 - val_accuracy: 0.6667\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.0520 - val_accuracy: 0.6667\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.0537 - val_accuracy: 0.6667\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.0554 - val_accuracy: 0.6667\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.0571 - val_accuracy: 0.6667\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.0588 - val_accuracy: 0.6667\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.0604 - val_accuracy: 0.6667\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.0621 - val_accuracy: 0.6667\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.0638 - val_accuracy: 0.6667\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.0654 - val_accuracy: 0.6667\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.0671 - val_accuracy: 0.6667\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.0687 - val_accuracy: 0.6667\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.0702 - val_accuracy: 0.6667\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.0718 - val_accuracy: 0.6667\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.0733 - val_accuracy: 0.6667\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.0749 - val_accuracy: 0.6667\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.0764 - val_accuracy: 0.6667\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.0780 - val_accuracy: 0.6667\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.0795 - val_accuracy: 0.6667\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.0810 - val_accuracy: 0.6667\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.0825 - val_accuracy: 0.6667\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.0840 - val_accuracy: 0.6667\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.0855 - val_accuracy: 0.6667\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.0869 - val_accuracy: 0.6667\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.0884 - val_accuracy: 0.6667\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.0899 - val_accuracy: 0.6667\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.0913 - val_accuracy: 0.6667\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.0927 - val_accuracy: 0.6667\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.0941 - val_accuracy: 0.6667\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.0956 - val_accuracy: 0.6667\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.0970 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, dec_in_train], y_train,\n",
    "    epochs=200,\n",
    "    batch_size=2,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1227bc98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSeq2Seq Translator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124meng_fra_translator.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Save tokenizers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSeq2Seq Translator\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124menglish_tokenizer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "model.save(r\"C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\\eng_fra_translator.h5\")\n",
    "\n",
    "# Save tokenizers\n",
    "with open(r\"C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\\english_tokenizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(eng_tokenizer, f)\n",
    "\n",
    "with open(r\"C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\\french_tokenizer.pkl\", 'wb') as f:\n",
    "    pickle.dump(fra_tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc51e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit([X_train, dec_in_train], y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit([X_train, dec_in_train], y_train, epochs=200, batch_size=2, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a72b3f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at eng_fra_translator.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng_fra_translator.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load tokenizers\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_tokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at eng_fra_translator.h5"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('eng_fra_translator.h5')\n",
    "\n",
    "# Load tokenizers\n",
    "with open('english_tokenizer.pkl', 'rb') as f:\n",
    "    eng_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('french_tokenizer.pkl', 'rb') as f:\n",
    "    fra_tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e7d979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ASUS\\\\Downloads\\\\Seq2Seq translator'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e233f965",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\\eng_fra_translator.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSeq2Seq Translator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng_fra_translator.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load tokenizers\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_tokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\\eng_fra_translator.h5"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "folder_path = r\"C:\\Users\\ASUS\\Downloads\\Seq2Seq Translator\"\n",
    "\n",
    "# Load model\n",
    "model = load_model(os.path.join(folder_path, 'eng_fra_translator.h5'))\n",
    "\n",
    "# Load tokenizers\n",
    "with open(os.path.join(folder_path, 'english_tokenizer.pkl'), 'rb') as f:\n",
    "    eng_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(folder_path, 'french_tokenizer.pkl'), 'rb') as f:\n",
    "    fra_tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f2664a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at c:\\Users\\ASUS\\Downloads\\Seq2Seq translator\\eng_fra_translator.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSeq2Seq translator\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng_fra_translator.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the tokenizers\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_tokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at c:\\Users\\ASUS\\Downloads\\Seq2Seq translator\\eng_fra_translator.h5"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "folder_path = r'c:\\Users\\ASUS\\Downloads\\Seq2Seq translator'\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(os.path.join(folder_path, 'eng_fra_translator.h5'))\n",
    "\n",
    "# Load the tokenizers\n",
    "with open(os.path.join(folder_path, 'english_tokenizer.pkl'), 'rb') as f:\n",
    "    eng_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(folder_path, 'french_tokenizer.pkl'), 'rb') as f:\n",
    "    fra_tokenizer = pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
